<!DOCTYPE html>
<html lang="en">
<head>
    {% block head %}  
    <link rel="stylesheet" href="/static/style.css" />
    <title>  Neil , Wercker, CI,  Kafka , Elastic Search, & Cloud Automation</title>  
    {% endblock %}
</head>
<body>  
        {% include "header.html" %}    

        {% include "menu.html" %}  

    <div id="content">
       
       {% block content %}
          <div class=superwelcome>
          Hey there friends! Do you want to do something really cool?<br><br>


          This little app I built is quite powerful and will let you laydown a Kafka as a service performance benchmark to test where you need to improve your infrastructure in order
          to prepare for the world of real time big data and IoT.  <br>
          
          <p class=subblurb>

          This app also exists to demonstrate my devops skills in various components within the stack.  It also exists to demonstrate my skills to put together complex architectures in 2-3 days ( a few hours each day ). This app is kicked off with a Docker I built at <a href="https://hub.docker.com/r/fed007/nginx-flask/" >DOCKER HUB REPO </a> and the app is supported by my git hub repo here:  <a href="https://github.com/fundagram/phabio" > GITHUB REPO </a>
          </p>
          <p class=important> This app will deploy a Kafka Node with ZooKeeper as our central nervous system along with an Elastic search node which will house the incoming data.  The incoming data will be processed by "Consumer Nodes"  that can be set to be multi-threaded to however many you want.
          </p>
          <p class=important>
          Additionally, feeding all this data into the system will be "Producer Nodes". These you will also create.  The Producer Nodes when created allow you to specify your payload / data files to insert into the system.  These large data sources should be one record per line and can be any json object format which will yeild an index decoder format injector for the future.   The producers can also multi-thread transport and will break and queue the supplied data source.
          <p>
          No special indexing or deciphering of data will take place.  Just raw msg entry to captur rae performance at a byte payload size.
          </p>
        <p>
          A single pub/sub stream ID will be used to push all data through.   The Producers can be set to loop or batch mode.   Once you have all the proper nodes deployed ( by clicking simple buttons in this app ) you will then get a "START" button on your Cluster dashboard which will then kick off all nodes to start processing, sending and receiving data.  
</p>
          <p class=secondary>
            The Dashboard will show you the various msg rates at the various points in your network and how you logical architecture may look.  In fact later on I'll look at a topology tool for merging logical capture points to physical node deployment points because everyone has a different architecture!
        </p>


          
          </div>

       {% endblock %}
     

    </div>
    <div id="footer">
        {% block footer %}      
        {% endblock %}
    </div>
</body>
</html>